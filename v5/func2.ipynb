{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec3e786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "#   Helper functions\n",
    "# -----------------------------\n",
    "\n",
    "def compute_Ai(x: np.ndarray, y: np.ndarray, w: np.ndarray, Ui_L: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute A_i = U_i^L + Σ_j w_ij[-y_j x_j^2 + (1+y_j)x_j] for all i.\"\"\"\n",
    "    term = w * ((1.0 + y) * x - y * x ** 2)  # broadcast over j\n",
    "    return Ui_L + term.sum(axis=1)\n",
    "\n",
    "\n",
    "def compute_Bi(x: np.ndarray, y: np.ndarray, w: np.ndarray,\n",
    "               Ui_L: np.ndarray, Ui_F: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute B_i = U_i^L + U_i^F + Σ_j w_ij[(1-y_j)x_j + y_j] for all i.\"\"\"\n",
    "    term = w * ((1.0 - y) * x + y)\n",
    "    return Ui_L + Ui_F + term.sum(axis=1)\n",
    "\n",
    "\n",
    "def compute_Lhat(x: np.ndarray, y: np.ndarray, w: np.ndarray,\n",
    "                 Ui_L: np.ndarray, Ui_F: np.ndarray, h: np.ndarray) -> float:\n",
    "    \"\"\"Evaluate the objective \\hat{L}(x, y).\"\"\"\n",
    "    Ai = compute_Ai(x, y, w, Ui_L)\n",
    "    Bi = compute_Bi(x, y, w, Ui_L, Ui_F)\n",
    "    return float(np.dot(h, Ai / Bi))\n",
    "\n",
    "# -----------------------------\n",
    "#   Gradients\n",
    "# -----------------------------\n",
    "\n",
    "def grad_x(x: np.ndarray, y: np.ndarray, w: np.ndarray,\n",
    "           Ui_L: np.ndarray, Ui_F: np.ndarray, h: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Gradient of \\hat{L} with respect to x (ascent direction).\"\"\"\n",
    "    Ai = compute_Ai(x, y, w, Ui_L)\n",
    "    Bi = compute_Bi(x, y, w, Ui_L, Ui_F)\n",
    "\n",
    "    dA_dx = w * ((1.0 + y) - 2.0 * y * x)     # shape (I, J)\n",
    "    dB_dx = w * (1.0 - y)\n",
    "\n",
    "    frac = (Bi[:, None] * dA_dx - Ai[:, None] * dB_dx) / (Bi[:, None] ** 2)\n",
    "    return (frac * h[:, None]).sum(axis=0)\n",
    "\n",
    "\n",
    "def grad_y(x: np.ndarray, y: np.ndarray, w: np.ndarray,\n",
    "           Ui_L: np.ndarray, Ui_F: np.ndarray, h: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Gradient of \\hat{L} with respect to y (descent direction).\"\"\"\n",
    "    Ai = compute_Ai(x, y, w, Ui_L)\n",
    "    Bi = compute_Bi(x, y, w, Ui_L, Ui_F)\n",
    "\n",
    "    dA_dy = w * (-x ** 2 + x)\n",
    "    dB_dy = w * (1.0 - x)\n",
    "\n",
    "    frac = (Bi[:, None] * dA_dy - Ai[:, None] * dB_dy) / (Bi[:, None] ** 2)\n",
    "    return (frac * h[:, None]).sum(axis=0)\n",
    "\n",
    "# -----------------------------\n",
    "#   Projection\n",
    "# -----------------------------\n",
    "\n",
    "def project_cardinality(v: np.ndarray, k: int, mask: np.ndarray | None = None) -> np.ndarray:\n",
    "    \"\"\"Project a vector onto the set {0,1}^J with at most *k* ones.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    v : array_like\n",
    "        Continuous scores for each coordinate.\n",
    "    k : int\n",
    "        Maximum number of 1s allowed.\n",
    "    mask : array_like of bool, optional\n",
    "        Positions that are forcibly set to 0 (e.g., columns already chosen by *x* when projecting *y*).\n",
    "    \"\"\"\n",
    "    v = np.clip(v, 0.0, 1.0)\n",
    "    if mask is not None:\n",
    "        v = v * (~mask)\n",
    "\n",
    "    if k >= v.size:\n",
    "        return (v > 0).astype(float)\n",
    "\n",
    "    # pick indices of the k largest values\n",
    "    idx = np.argpartition(-v, k)[:k]\n",
    "    out = np.zeros_like(v)\n",
    "    out[idx] = 1.0\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "#   OGDA Solver (with history)\n",
    "# -----------------------------\n",
    "\n",
    "def ogda_solver(x0: np.ndarray, y0: np.ndarray, w: np.ndarray,\n",
    "                Ui_L: np.ndarray, Ui_F: np.ndarray, h: np.ndarray,\n",
    "                p: int, r: int, *, eta: float = 0.01, max_iter: int = 500,\n",
    "                tol: float = 1e-6, return_history: bool = False):\n",
    "    \"\"\"Optimistic Gradient Descent–Ascent (OGDA) solver that **stores full history**.\n",
    "\n",
    "    The function now always records objective value, ||dx|| and ||dy|| for each\n",
    "    iteration so that the convergence process can be plotted easily.\n",
    "    Set *return_history=True* to retrieve the history dictionary.\n",
    "    \"\"\"\n",
    "    x_prev = x0.copy()\n",
    "    y_prev = y0.copy()\n",
    "\n",
    "    # Initial gradients\n",
    "    gx_prev = grad_x(x_prev, y_prev, w, Ui_L, Ui_F, h)\n",
    "    gy_prev = grad_y(x_prev, y_prev, w, Ui_L, Ui_F, h)\n",
    "\n",
    "    obj_vals, dx_vals, dy_vals = [], [], []\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # Current gradients\n",
    "        gx = grad_x(x_prev, y_prev, w, Ui_L, Ui_F, h)\n",
    "        gy = grad_y(x_prev, y_prev, w, Ui_L, Ui_F, h)\n",
    "\n",
    "        # OGDA update (predictor–corrector style)\n",
    "        x_tmp = x_prev + eta * (2.0 * gx - gx_prev)  # ascent step for x\n",
    "        y_tmp = y_prev - eta * (2.0 * gy - gy_prev)  # descent step for y\n",
    "\n",
    "        # Projection to maintain feasibility\n",
    "        x_next = project_cardinality(x_tmp, p)\n",
    "        y_next = project_cardinality(y_tmp, r, mask=(x_next > 0))\n",
    "\n",
    "        # Metrics for this iteration\n",
    "        dx = np.linalg.norm(x_next - x_prev)\n",
    "        dy = np.linalg.norm(y_next - y_prev)\n",
    "        obj = compute_Lhat(x_next, y_next, w, Ui_L, Ui_F, h)\n",
    "\n",
    "        obj_vals.append(obj)\n",
    "        dx_vals.append(dx)\n",
    "        dy_vals.append(dy)\n",
    "\n",
    "        # Convergence check\n",
    "        if max(dx, dy) < tol:\n",
    "            break\n",
    "\n",
    "        # Prepare for next iteration\n",
    "        gx_prev, gy_prev = gx, gy\n",
    "        x_prev, y_prev = x_next, y_next\n",
    "\n",
    "    history = {\n",
    "        \"objective\": np.array(obj_vals),\n",
    "        \"dx\": np.array(dx_vals),\n",
    "        \"dy\": np.array(dy_vals)\n",
    "    }\n",
    "\n",
    "    if return_history:\n",
    "        return x_next, y_next, history\n",
    "    return x_next, y_next\n",
    "\n",
    "# -----------------------------\n",
    "#   Plot helper\n",
    "# -----------------------------\n",
    "\n",
    "def plot_history(history: dict[str, np.ndarray], *, logy: bool = True):\n",
    "    \"\"\"Quick plotting utility for convergence curves.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    history : dict\n",
    "        Dictionary returned by *ogda_solver(return_history=True)*.\n",
    "    logy : bool, default True\n",
    "        Plot the y‑axis in log scale for dx/dy curves.\n",
    "    \"\"\"\n",
    "    iters = np.arange(1, len(history[\"objective\"]) + 1)\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "    # Objective value on primary axis\n",
    "    # ax1.plot(iters, history[\"objective\"], label=\"objective\", linewidth=1.5)\n",
    "    ax1.set_xlabel(\"iteration\")\n",
    "    ax1.set_ylabel(\"objective\")\n",
    "\n",
    "    # dx & dy on secondary axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(iters, history[\"dx\"], linestyle=\"--\", label=\"‖dx‖\")\n",
    "    ax2.plot(iters, history[\"dy\"], linestyle=\":\", label=\"‖dy‖\")\n",
    "    ax2.set_ylabel(\"step size\")\n",
    "    if logy:\n",
    "        ax2.set_yscale(\"log\")\n",
    "\n",
    "    # Legends\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    l2, lab2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines + l2, labels + lab2, loc=\"best\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c38f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "#   Helper functions\n",
    "# -----------------------------\n",
    "\n",
    "def compute_Ai(x: np.ndarray, y: np.ndarray, w: np.ndarray, Ui_L: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute A_i = U_i^L + Σ_j w_ij[-y_j x_j^2 + (1+y_j)x_j] for all i.\"\"\"\n",
    "    term = w * ((1.0 + y) * x - y * x ** 2)  # broadcast over j\n",
    "    return Ui_L + term.sum(axis=1)\n",
    "\n",
    "\n",
    "def compute_Bi(x: np.ndarray, y: np.ndarray, w: np.ndarray,\n",
    "               Ui_L: np.ndarray, Ui_F: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute B_i = U_i^L + U_i^F + Σ_j w_ij[(1-y_j)x_j + y_j] for all i.\"\"\"\n",
    "    term = w * ((1.0 - y) * x + y)\n",
    "    return Ui_L + Ui_F + term.sum(axis=1)\n",
    "\n",
    "\n",
    "def compute_Lhat(x: np.ndarray, y: np.ndarray, w: np.ndarray,\n",
    "                 Ui_L: np.ndarray, Ui_F: np.ndarray, h: np.ndarray) -> float:\n",
    "    \"\"\"Evaluate the objective \\hat{L}(x, y).\"\"\"\n",
    "    Ai = compute_Ai(x, y, w, Ui_L)\n",
    "    Bi = compute_Bi(x, y, w, Ui_L, Ui_F)\n",
    "    return float(np.dot(h, Ai / Bi))\n",
    "\n",
    "# -----------------------------\n",
    "#   Gradients\n",
    "# -----------------------------\n",
    "\n",
    "def grad_x(x: np.ndarray, y: np.ndarray, w: np.ndarray,\n",
    "           Ui_L: np.ndarray, Ui_F: np.ndarray, h: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Gradient of \\hat{L} with respect to x (ascent direction).\"\"\"\n",
    "    Ai = compute_Ai(x, y, w, Ui_L)\n",
    "    Bi = compute_Bi(x, y, w, Ui_L, Ui_F)\n",
    "\n",
    "    dA_dx = w * ((1.0 + y) - 2.0 * y * x)     # shape (I, J)\n",
    "    dB_dx = w * (1.0 - y)\n",
    "\n",
    "    frac = (Bi[:, None] * dA_dx - Ai[:, None] * dB_dx) / (Bi[:, None] ** 2)\n",
    "    return (frac * h[:, None]).sum(axis=0)\n",
    "\n",
    "\n",
    "def grad_y(x: np.ndarray, y: np.ndarray, w: np.ndarray,\n",
    "           Ui_L: np.ndarray, Ui_F: np.ndarray, h: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Gradient of \\hat{L} with respect to y (descent direction).\"\"\"\n",
    "    Ai = compute_Ai(x, y, w, Ui_L)\n",
    "    Bi = compute_Bi(x, y, w, Ui_L, Ui_F)\n",
    "\n",
    "    dA_dy = w * (-x ** 2 + x)\n",
    "    dB_dy = w * (1.0 - x)\n",
    "\n",
    "    frac = (Bi[:, None] * dA_dy - Ai[:, None] * dB_dy) / (Bi[:, None] ** 2)\n",
    "    return (frac * h[:, None]).sum(axis=0)\n",
    "\n",
    "# -----------------------------\n",
    "#   Projection\n",
    "# -----------------------------\n",
    "\n",
    "def project_cardinality(v: np.ndarray, k: int, mask: np.ndarray | None = None) -> np.ndarray:\n",
    "    \"\"\"Project a vector onto the set {0,1}^J with at most *k* ones.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    v : array_like\n",
    "        Continuous scores for each coordinate.\n",
    "    k : int\n",
    "        Maximum number of 1s allowed.\n",
    "    mask : array_like of bool, optional\n",
    "        Positions that are forcibly set to 0 (e.g., columns already chosen by *x* when projecting *y*).\n",
    "    \"\"\"\n",
    "    v = np.clip(v, 0.0, 1.0)\n",
    "    if mask is not None:\n",
    "        v = v * (~mask)\n",
    "\n",
    "    if k >= v.size:\n",
    "        return (v > 0).astype(float)\n",
    "\n",
    "    # pick indices of the k largest values\n",
    "    idx = np.argpartition(-v, k)[:k]\n",
    "    out = np.zeros_like(v)\n",
    "    out[idx] = 1.0\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "#   OGDA Solver\n",
    "# -----------------------------\n",
    "\n",
    "def ogda_solver(x0: np.ndarray, y0: np.ndarray, w: np.ndarray,\n",
    "                Ui_L: np.ndarray, Ui_F: np.ndarray, h: np.ndarray,\n",
    "                p: int, r: int, *, eta: float = 0.01, max_iter: int = 500,\n",
    "                tol: float = 1e-6, return_history: bool = False):\n",
    "    \"\"\"Optimistic Gradient Descent–Ascent (OGDA) solver for the bilevel game.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x0, y0 : array_like (J,)\n",
    "        Initial continuous vectors (will be projected to 0/1 at every step).\n",
    "    w, Ui_L, Ui_F, h : arrays from the original model.\n",
    "    p, r : int\n",
    "        Cardinality constraints for x and y respectively.\n",
    "    eta : float, default 0.01\n",
    "        Step size.\n",
    "    max_iter : int, default 500\n",
    "        Maximum number of OGDA iterations.\n",
    "    tol : float, default 1e-6\n",
    "        Convergence tolerance on (x, y) change.\n",
    "    return_history : bool, default False\n",
    "        If True, also return a list with (Lhat, ||dx||, ||dy||) per iteration.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_opt, y_opt : ndarray\n",
    "        Binary solutions after convergence.\n",
    "    history : list | None\n",
    "        Only if *return_history* is True.\n",
    "    \"\"\"\n",
    "    x_prev = x0.copy()\n",
    "    y_prev = y0.copy()\n",
    "\n",
    "    # Initial gradients\n",
    "    gx_prev = grad_x(x_prev, y_prev, w, Ui_L, Ui_F, h)\n",
    "    gy_prev = grad_y(x_prev, y_prev, w, Ui_L, Ui_F, h)\n",
    "\n",
    "    hist = []\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # Current gradients\n",
    "        gx = grad_x(x_prev, y_prev, w, Ui_L, Ui_F, h)\n",
    "        gy = grad_y(x_prev, y_prev, w, Ui_L, Ui_F, h)\n",
    "\n",
    "        # OGDA update (predictor–corrector style)\n",
    "        x_tmp = x_prev + eta * (2.0 * gx - gx_prev)  # ascent step for x\n",
    "        y_tmp = y_prev - eta * (2.0 * gy - gy_prev)  # descent step for y\n",
    "\n",
    "        # Projection to maintain feasibility\n",
    "        x_next = project_cardinality(x_tmp, p)\n",
    "        y_next = project_cardinality(y_tmp, r, mask=(x_next > 0))\n",
    "\n",
    "        # Convergence check\n",
    "        dx = np.linalg.norm(x_next - x_prev)\n",
    "        dy = np.linalg.norm(y_next - y_prev)\n",
    "\n",
    "        if return_history:\n",
    "            L_val = compute_Lhat(x_next, y_next, w, Ui_L, Ui_F, h)\n",
    "            hist.append((L_val, dx, dy))\n",
    "\n",
    "        if max(dx, dy) < tol:\n",
    "            break\n",
    "\n",
    "        # Prepare for next iteration\n",
    "        gx_prev, gy_prev = gx, gy\n",
    "        x_prev, y_prev = x_next, y_next\n",
    "\n",
    "    if return_history:\n",
    "        return x_next, y_next, hist\n",
    "    return x_next, y_next\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

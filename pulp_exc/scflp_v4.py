# scflp_fn.py
# é€æ¬¡å‹ ç«¶äº‰çš„æ–½è¨­é…ç½®å•é¡Œ (S-CFLP) â€” é–¢æ•°ç‰ˆï¼ˆè«–æ–‡ Algorithm 1 ã«å¿ å®Ÿï¼‰
# ãƒ»Branch-and-(outer)-Cut + é…å»¶åˆ¶ç´„ç”Ÿæˆï¼ˆDCGï¼‰
# ãƒ»åˆ†é›¢ã¯ã€Œè¿‘ä¼¼ã€(å‘½é¡Œ5) ã¾ãŸã¯ã€Œå³å¯†ã€ã‚’é¸æŠå¯ï¼ˆæ—¢å®š: è¿‘ä¼¼ï¼‰
# ãƒ»ã‚«ãƒƒãƒˆã¯ 1 å›ã®é•åã«ã¤ã 1 æœ¬ã®ã¿è¿½åŠ ï¼ˆBulge ã‹ Submodularï¼‰
# ãƒ»MILP ã¯ PuLP(CBC/GLPK)

from __future__ import annotations

import time
import shutil
import numpy as np
from typing import Dict, List, Optional, Tuple, Set

# ========== ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ»å‰å‡¦ç† ==================================================


def normalize_h(h: np.ndarray) -> np.ndarray:
    s = float(h.sum())
    return h / s if abs(s - 1.0) > 1e-12 else h


def scflp_from_coordinates(
    demand_xy: np.ndarray,
    cand_xy: np.ndarray,
    alpha: np.ndarray,
    beta: float,
    p: int,
    r: int,
    leader_xy: Optional[np.ndarray] = None,
    follower_xy: Optional[np.ndarray] = None,
    h: Optional[np.ndarray] = None,
) -> Dict[str, object]:
    """2æ¬¡å…ƒåº§æ¨™ã‹ã‚‰S-CFLPãƒ‡ãƒ¼ã‚¿(dict)ã‚’ä½œã‚‹ã€‚"""
    I, J = demand_xy.shape[0], cand_xy.shape[0]
    h = normalize_h(np.ones(I) / I if h is None else h.astype(float))
    alpha = alpha.reshape(J)

    def pairwise_dist(A, B):
        return np.sqrt(((A[:, None, :] - B[None, :, :]) ** 2).sum(axis=2))

    d_cand = pairwise_dist(demand_xy, cand_xy)
    w = np.exp(alpha.reshape(1, J) - beta * d_cand)

    UL = np.zeros(I)
    UF = np.zeros(I)
    if leader_xy is not None and len(leader_xy) > 0:
        dL = pairwise_dist(demand_xy, leader_xy)
        wL = np.exp(alpha.mean() - beta * dL)
        UL = wL.sum(axis=1)
    if follower_xy is not None and len(follower_xy) > 0:
        dF = pairwise_dist(demand_xy, follower_xy)
        wF = np.exp(alpha.mean() - beta * dF)
        UF = wF.sum(axis=1)

    return dict(h=h, UL=UL, UF=UF, w=w, I=I, J=J, p=int(p), r=int(r))


def make_random_instance(
    I: int = 10,
    J: int = 10,
    p: int = 2,
    r: int = 2,
    beta: float = 0.1,
    alpha_mean: float = 0.0,
    alpha_std: float = 0.0,
    seed: int = 0,
) -> Dict[str, object]:
    rng = np.random.default_rng(seed)
    demand_xy = rng.uniform(0, 50, size=(I, 2))
    cand_xy = rng.uniform(0, 50, size=(J, 2))
    alpha = rng.normal(loc=alpha_mean, scale=alpha_std, size=(J,))
    return scflp_from_coordinates(demand_xy, cand_xy, alpha, beta, p, r)


# ========== ç›®çš„é–¢æ•°ãƒ»è©•ä¾¡ãƒ»åˆ†é›¢ãƒ»ã‚«ãƒƒãƒˆä¿‚æ•° ====================================


def L_value(data: Dict[str, object], x: np.ndarray, y: np.ndarray) -> float:
    """å¼(4)ã®çœŸå€¤ L(x,y)ã€‚"""
    w = data["w"]
    UL = data["UL"]
    UF = data["UF"]
    h = data["h"]
    x = x.reshape(-1)
    y = y.reshape(-1)
    xy = np.maximum(x, y)
    num = UL + w.dot(x)
    den = UL + UF + w.dot(xy)
    return float((h * (num / den)).sum())


def Lb_and_grad(
    data: Dict[str, object], x: np.ndarray, y: np.ndarray
) -> Tuple[float, np.ndarray]:
    """Bulgeé–¢æ•° L_b(x,y) ã¨ xå‹¾é…ï¼ˆå¼(11)ã®æ¥å¹³é¢ï¼‰ã€‚"""
    w = data["w"]
    UL = data["UL"]
    UF = data["UF"]
    h = data["h"]
    J = w.shape[1]
    x = x.reshape(J)
    y = y.reshape(J)
    one_minus_y = 1.0 - y
    P = UL + UF + w.dot(one_minus_y * x + y)  # (I,)
    Q = UL + w.dot(-y * (x**2) + (1.0 + y) * x)  # (I,)
    Lb = (h * (Q / P)).sum()
    grad = np.zeros(J)
    P2 = P * P
    for j in range(J):
        wj = w[:, j]
        term = (
            -wj * one_minus_y[j] * Q / P2 + wj * (-2.0 * y[j] * x[j] + 1.0 + y[j]) / P
        )
        grad[j] = (h * term).sum()
    return float(Lb), grad


def submodular_cut_coeffs(
    data: Dict[str, object], x_bin: np.ndarray, y_bin: np.ndarray
) -> Tuple[float, Dict[int, float]]:
    """Nemhauserâ€“Wolsey ç·šå½¢åŒ–ã®ä¿‚æ•°ï¼ˆå›ºå®šYä¸‹ã€å¼(8)ï¼‰ã€‚"""
    w = data["w"]
    UL = data["UL"]
    UF = data["UF"]
    h = data["h"]
    J = w.shape[1]
    S: Set[int] = set(np.where(x_bin > 0.5)[0].tolist())
    Y: Set[int] = set(np.where(y_bin > 0.5)[0].tolist())

    def L_Y_of_X(Xset: Set[int]) -> float:
        maskX = np.zeros(J)
        if Xset:
            maskX[list(Xset)] = 1.0
        maskXUY = maskX
        if Y:
            maskY = np.zeros(J)
            maskY[list(Y)] = 1.0
            maskXUY = np.maximum(maskX, maskY)
        num = UL + w.dot(maskX)
        den = UL + UF + w.dot(maskXUY)
        return float((h * (num / den)).sum())

    LYS = L_Y_of_X(S)

    rho_S_k: Dict[int, float] = {}
    for k in range(J):
        if k in S:
            continue
        Sk = set(S)
        Sk.add(k)
        rho_S_k[k] = L_Y_of_X(Sk) - LYS

    rho_Jminus_k_k: Dict[int, float] = {}
    Jall = set(range(J))
    for k in range(J):
        full_minus_k = set(Jall)
        full_minus_k.remove(k)
        LY_full_minus_k = L_Y_of_X(full_minus_k)
        rho_Jminus_k_k[k] = L_Y_of_X(Jall) - LY_full_minus_k

    c0 = LYS - sum(rho_Jminus_k_k[k] for k in S)
    coeffs: Dict[int, float] = {}
    for k in S:
        coeffs[k] = rho_Jminus_k_k[k]
    for k in range(J):
        if k not in S:
            coeffs[k] = rho_S_k[k]
    return c0, coeffs


def approximate_separation_y(
    data: Dict[str, object], x: np.ndarray
) -> Tuple[np.ndarray, float, np.ndarray]:
    """å‘½é¡Œ5ã®è¿‘ä¼¼åˆ†é›¢ã€‚y_hat, ub_lin, beta ã‚’è¿”ã™ã€‚"""
    w = data["w"]
    UL = data["UL"]
    UF = data["UF"]
    h = data["h"]
    r = data["r"]
    I, J = w.shape
    x = x.reshape(J)
    a = UL + w.dot(x)  # (I,)
    w_scaled = w * (1.0 - x.reshape(1, J))  # (I,J)

    smallest = (
        np.partition(w_scaled, r - 1, axis=1)[:, :r].sum(axis=1)
        if r > 0
        else np.zeros(I)
    )
    largest = (
        (-np.partition(-w_scaled, r - 1, axis=1)[:, :r]).sum(axis=1)
        if r > 0
        else np.zeros(I)
    )
    wL = UF + smallest
    wU = UF + largest

    denomL = a + wL
    denomU = a + wU
    alpha_terms = a * (a + wU + wL - UF) / (denomU * denomL)
    alpha_val = float((h * alpha_terms).sum())
    common = h * (a / (denomU * denomL))
    beta = (common.reshape(I, 1) * (w * (1.0 - x.reshape(1, J)))).sum(axis=0)

    idx_sorted = np.argsort(-beta)
    y_hat = np.zeros(J)
    if r > 0:
        y_hat[idx_sorted[:r]] = 1.0
    ub_lin = alpha_val - float(beta[y_hat > 0.5].sum())
    return y_hat, ub_lin, beta


# ========== MILPï¼ˆPuLPï¼‰ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========================================


def build_master_pulp(nJ, p, solver_name="CBC", lp_relax=True):
    import pulp

    model = pulp.LpProblem("S_CFLP_Master", pulp.LpMaximize)
    cat = "Continuous" if lp_relax else "Binary"
    x = {
        j: pulp.LpVariable(f"x[{j}]", lowBound=0, upBound=1, cat=cat) for j in range(nJ)
    }
    theta = pulp.LpVariable("theta", lowBound=0.0, upBound=1.0, cat="Continuous")
    model += pulp.lpSum([x[j] for j in range(nJ)]) == p, "budget_eq"
    model += theta
    solver = (
        pulp.PULP_CBC_CMD(msg=False)
        if solver_name == "CBC"
        else pulp.GLPK_CMD(msg=False)
    )
    return model, x, theta, solver


def solve_master(model, solver) -> int:
    return int(model.solve(solver))


def get_values_pulp(xvars: Dict[int, object], theta) -> Tuple[np.ndarray, float]:
    x_hat = np.array([float(v.value()) for j, v in sorted(xvars.items())], dtype=float)
    return x_hat, float(theta.value())


def add_linear_cut_theta_le(
    model, theta, c0: float, coeffs: Dict[int, float], name: str
):
    """theta <= c0 + sum_j coeffs_j x_j  ã‚’  theta - sum <= c0 ã§è¿½åŠ """
    expr = 0.0
    for j, cj in coeffs.items():
        expr += cj * model.variablesDict()[f"x[{j}]"]
    model += (theta - expr <= c0), name


# ========== ãƒ­ã‚° ===============================================================


def _term_width(default=100) -> int:
    try:
        return shutil.get_terminal_size((default, 20)).columns
    except Exception:
        return default


def log_header(title: str, level: str):
    if level not in ("info", "debug"):
        return
    w = min(_term_width(), 80)
    print("â•" * w)
    print(f"ğŸš€ {title}")
    print("â•" * w)


def log_line(level: str, char="â”€"):
    if level not in ("info", "debug"):
        return
    print(char * min(_term_width(), 80))


def log_step(msg: str, level: str):
    if level not in ("info", "debug"):
        return
    print(f"â¤ {msg}")


def log_debug(msg: str, level: str):
    if level != "debug":
        return
    print(f"   Â· {msg}")


def log_cut(kind: str, level: str):
    if level not in ("info", "debug"):
        return
    print(f"âœ‚ï¸  ã‚«ãƒƒãƒˆè¿½åŠ : {kind.upper()}")


def log_success(msg: str, level: str):
    if level not in ("info", "debug"):
        return
    print(f"âœ… {msg}")


def log_warn(msg: str, level: str):
    if level not in ("info", "debug"):
        return
    print(f"âš ï¸  {msg}")


def log_done(msg: str, tstart: float, level: str):
    if level not in ("info", "debug"):
        return
    w = min(_term_width(), 80)
    print(f"ğŸ {msg}ï¼ˆçµŒé {time.time() - tstart:.2f}sï¼‰")
    print("â•" * w)


# ========== Outer Cutting-Planeï¼ˆè«–æ–‡ è¡Œ1,5â€“9 éƒ¨åˆ†ï¼‰ ===========================


def scflp_solve(
    data: Dict[str, object],
    max_rounds: int = 200,
    tol: float = 1e-12,
    pulp_solver: str = "CBC",
    log_level: str = "info",
    separation: str = "approx",  # "approx" or "exact"ï¼ˆexactã¯æœªå®Ÿè£…ï¼‰
    cut_policy: str = "auto",  # "bulge" / "submodular" / "auto"
) -> Dict[str, object]:
    """
    è«–æ–‡ã®å¤–å´ cutting-planeï¼ˆåˆ†æãªã—ï¼‰ã€‚é•åãŒã‚ã‚‹é™ã‚Šã€1 å›ã« 1 æœ¬ã®ã‚«ãƒƒãƒˆã‚’è¿½åŠ ã€‚
    """
    J = int(data["J"])
    I = int(data["I"])
    p = int(data["p"])
    r = int(data["r"])

    model, xvars, theta, solver = build_master_pulp(J, p, solver_name=pulp_solver)
    cuts_added = 0
    cuts_log: List[Tuple[str, float]] = []
    t0 = time.time()
    bulge_count = 0
    submod_count = 0

    log_header("S-CFLP ã‚’è§£ãã¾ã™ï¼ˆOuter Cutting-Planeï¼‰", log_level)
    log_step(f"å€™è£œåœ° J = {J}, éœ€è¦ç‚¹ I = {I}, p = {p}, r = {r}", log_level)

    for it in range(1, max_rounds + 1):
        log_line(log_level)
        status = solve_master(model, solver)
        x_hat, theta_hat = get_values_pulp(xvars, theta)
        is_int = bool(np.all((x_hat < 1e-6) | (x_hat > 1 - 1e-6)))
        log_step(
            f"ğŸ§® ãƒ©ã‚¦ãƒ³ãƒ‰ {it}: Î¸Ì‚ = {theta_hat:.6f}, |x| = {x_hat.sum():.0f}/{p}ï¼ˆ{'æ•´æ•°' if is_int else 'å°æ•°æ··åœ¨'}ï¼‰",
            log_level,
        )

        # åˆ†é›¢ï¼ˆæ—¢å®š: è¿‘ä¼¼ï¼‰
        if separation == "approx":
            y_hat, ub_lin, beta = approximate_separation_y(data, x_hat)
        else:
            # exact åˆ†é›¢(6)ãŒæœªå®Ÿè£…ã®å ´åˆã‚‚ API ã‚’ä¿ã¤
            y_hat, ub_lin, beta = approximate_separation_y(data, x_hat)

        true_val = L_value(data, x_hat, y_hat)
        gap = theta_hat - true_val
        y_idx = np.where(y_hat > 0.5)[0].tolist()
        log_step(f"ğŸ” åˆ†é›¢: yÌ‚ = {y_idx}ï¼ˆr={r}ï¼‰, ä¸Šç•Œ Î±âˆ’Î²áµ€yÌ‚ = {ub_lin:.6f}", log_level)
        log_step(f"ğŸ“ è©•ä¾¡: L(xÌ‚,yÌ‚) = {true_val:.6f} â†’ é•å {gap:.3e}", log_level)

        # è«–æ–‡ è¡Œ7: é•åã‚ã‚Šãªã‚‰ 1 æœ¬ã ã‘ã‚«ãƒƒãƒˆè¿½åŠ 
        if theta_hat > true_val + tol:
            kind_used = None

            # ã‚«ãƒƒãƒˆé¸æŠ
            policy = cut_policy.lower()
            if policy == "submodular" or (policy == "auto" and is_int):
                # x ãŒæ•´æ•°ãªã‚‰ Submodularï¼ˆauto ã§ã‚‚ã“ã“ã«æ¥ã‚‹ï¼‰
                x_bin = (x_hat > 0.5).astype(float)
                y_bin = (y_hat > 0.5).astype(float)
                c0, coeffs = submodular_cut_coeffs(data, x_bin, y_bin)
                add_linear_cut_theta_le(
                    model, theta, c0, coeffs, f"submod_{cuts_added}"
                )
                kind_used = "submod"
                submod_count += 1
            elif policy == "bulge" or (policy == "auto" and not is_int):
                # åˆ†æ•°ã®é–“ã¯ Bulgeï¼ˆauto ã§ã‚‚ã“ã“ï¼‰
                Lb, grad = Lb_and_grad(data, x_hat, y_hat)
                c0 = Lb - float((grad * x_hat).sum())
                coeffs = {j: float(grad[j]) for j in range(J) if abs(grad[j]) > 1e-12}
                add_linear_cut_theta_le(model, theta, c0, coeffs, f"bulge_{cuts_added}")
                kind_used = "bulge"
                bulge_count += 1
            else:
                raise ValueError(f"unknown cut_policy: {policy}")

            cuts_added += 1
            cuts_log.append((kind_used, c0))
            log_cut(kind_used, log_level)
            continue  # å¼·åŒ–ã—ãŸåŒä¸€ãƒ•ã‚©ãƒ¼ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å†è§£ãï¼ˆè¡Œ9ï¼‰
        else:
            log_success("é•åãªã—ã€‚ç¾åœ¨ã® Y-closure ã«å¯¾ã—ã¦æœ€é©ã§ã™ã€‚", log_level)
            break

    # ä»•ä¸Šã’
    status = solve_master(model, solver)
    x_hat, theta_hat = get_values_pulp(xvars, theta)
    sel = np.where(x_hat > 0.5)[0].tolist()
    log_done("æ±‚è§£å®Œäº†", t0, log_level)
    log_step(f"âœ¨ ç›®çš„å€¤ Î¸ = {theta_hat:.6f} / é¸æŠã‚µã‚¤ãƒˆ {sel}", log_level)
    log_step(
        f"ğŸ§· è¿½åŠ ã‚«ãƒƒãƒˆ åˆè¨ˆ {cuts_added} æœ¬ï¼ˆBulge: {bulge_count}, Submodular: {submod_count}ï¼‰",
        log_level,
    )
    if abs(x_hat.sum() - p) > 1e-6:
        log_warn("Î£x ãŒ p ã¨ä¸€è‡´ã—ã¦ã„ã¾ã›ã‚“ã€‚äºˆç®—åˆ¶ç´„ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚", log_level)

    return dict(
        x=x_hat,
        theta=theta_hat,
        obj=theta_hat,
        status=status,
        iterations=it,
        cuts=cuts_added,
        cuts_log=cuts_log,
        bulge_cuts=bulge_count,
        submod_cuts=submod_count,
        selected_sites=sel,
        time_sec=time.time() - t0,
    )


# ========== åˆ†æä»˜ã Branch-and-Cutï¼šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ============================


def build_master_with_fixings_and_cuts(
    nJ: int,
    p: int,
    fix1: Set[int],
    fix0: Set[int],
    local_cuts: List[Tuple[float, Dict[int, float]]],
    global_cuts: List[Tuple[float, Dict[int, float]]],
    solver_name: str = "CBC",
    lp_relax: bool = True,
):
    """
    åˆ†æã§ã®å›ºå®šã¨æ—¢å­˜ã‚«ãƒƒãƒˆã‚’å«ã‚ã¦ master ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
    è«–æ–‡ è¡Œ8: ã€Œå…¨ãƒ•ã‚©ãƒ¼ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€ã«åŠ¹ãã‚ˆã†ã€global_cuts ã‚‚æŠ•å…¥ã€‚
    """
    import pulp

    model = pulp.LpProblem("S_CFLP_Master", pulp.LpMaximize)
    cat = "Continuous" if lp_relax else "Binary"
    xvars = {}
    for j in range(nJ):
        if j in fix1:
            xvars[j] = pulp.LpVariable(
                f"x[{j}]", lowBound=1, upBound=1, cat="Continuous"
            )  # å›ºå®š
        elif j in fix0:
            xvars[j] = pulp.LpVariable(
                f"x[{j}]", lowBound=0, upBound=0, cat="Continuous"
            )  # å›ºå®š
        else:
            xvars[j] = pulp.LpVariable(f"x[{j}]", lowBound=0, upBound=1, cat=cat)
    theta = pulp.LpVariable("theta", lowBound=0.0, upBound=1.0, cat="Continuous")
    model += pulp.lpSum([xvars[j] for j in range(nJ)]) == p, "budget_eq"
    model += theta

    # æ—¢å­˜ã‚«ãƒƒãƒˆï¼ˆlocal + globalï¼‰ã‚’å†è¿½åŠ 
    def replay(mdl, cuts, prefix):
        for k, (c0, coeffs) in enumerate(cuts):
            expr = 0.0
            for j, cj in coeffs.items():
                expr += cj * xvars[j]
            mdl += (theta - expr <= c0), f"{prefix}_{k}"

    replay(model, global_cuts, "gcut")
    replay(model, local_cuts, "lcut")

    if solver_name == "CBC":
        solver = pulp.PULP_CBC_CMD(msg=False)
    elif solver_name == "GLPK":
        solver = pulp.GLPK_CMD(msg=False)
    else:
        solver = pulp.PULP_CBC_CMD(msg=False)

    return model, xvars, theta, solver


def choose_branch_variable(x_hat: np.ndarray) -> Optional[int]:
    """æœ€ã‚‚ 0.5 ã«è¿‘ã„åˆ†æ•°å¤‰æ•°ã§åˆ†æã€‚"""
    frac = np.where((x_hat > 1e-6) & (x_hat < 1 - 1e-6))[0]
    if len(frac) == 0:
        return None
    j_star = int(frac[np.argmin(np.abs(x_hat[frac] - 0.5))])
    return j_star


def make_node(
    fix1: Optional[Set[int]] = None,
    fix0: Optional[Set[int]] = None,
    cuts: Optional[List[Tuple[float, Dict[int, float]]]] = None,
    depth: int = 0,
):
    """åˆ†æãƒãƒ¼ãƒ‰ï¼ˆéƒ¨åˆ†å•é¡Œï¼‰ã‚’ä½œã‚‹ç°¡æ˜“ãƒ•ã‚¡ã‚¯ãƒˆãƒªã€‚"""
    return dict(
        fix1=set() if fix1 is None else set(fix1),
        fix0=set() if fix0 is None else set(fix0),
        cuts=[] if cuts is None else list(cuts),
        depth=int(depth),
        ub=float("-inf"),  # ç›´è¿‘ Î¸Ì‚
    )


# ========== 1ãƒãƒ¼ãƒ‰å†…ã® cutting-planeï¼ˆé•åãŒç„¡ããªã‚‹ã¾ã§ï¼‰ ====================


def node_solve_until_no_violation(
    data: Dict[str, object],
    node: Dict[str, object],
    global_cuts: List[Tuple[float, Dict[int, float]]],
    pulp_solver: str,
    tol: float,
    cut_policy: str = "auto",  # "bulge"/"submodular"/"auto"
    separation: str = "approx",  # "approx"/"exact"
    max_cuts_per_node: int = 200,
    log_level: str = "info",
    lp_relax: bool = True,
):
    """
    è«–æ–‡ è¡Œ5â€“9: 1ãƒãƒ¼ãƒ‰ã®ãƒ•ã‚©ãƒ¼ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’
    é•åãŒãªããªã‚‹ã¾ã§å¼·åŒ–ã—ã¦è§£ãï¼ˆæ¯å›ã‚«ãƒƒãƒˆã¯ 1 æœ¬ã®ã¿è¿½åŠ ï¼‰ã€‚
    """
    J = int(data["J"])
    total_new_cuts = 0
    bulge_added = 0
    submod_added = 0

    for _ in range(max_cuts_per_node):
        model, xvars, theta, solver = build_master_with_fixings_and_cuts(
            nJ=J,
            p=int(data["p"]),
            fix1=node["fix1"],
            fix0=node["fix0"],
            local_cuts=node["cuts"],
            global_cuts=global_cuts,
            solver_name=pulp_solver,
            lp_relax=lp_relax,
        )
        status = solve_master(model, solver)
        x_hat, theta_hat = get_values_pulp(xvars, theta)
        is_int = bool(np.all((x_hat < 1e-6) | (x_hat > 1 - 1e-6)))

        # åˆ†é›¢
        if separation == "approx":
            y_hat, _, _ = approximate_separation_y(data, x_hat)
        else:
            y_hat, _, _ = approximate_separation_y(data, x_hat)
        true_val = L_value(data, x_hat, y_hat)
        gap = theta_hat - true_val

        log_step(
            f"   Â· ãƒãƒ¼ãƒ‰(depth={node['depth']}): Î¸Ì‚={theta_hat:.6f}, L={true_val:.6f}, viol={gap:.3e}",
            log_level,
        )

        # é•åãªã— â†’ çµ‚äº†ï¼ˆè¡Œ10ä»¥é™ã¯ä¸Šä½ã§åˆ†å²åˆ¤æ–­ï¼‰
        if theta_hat <= true_val + tol:
            node["ub"] = float(theta_hat)
            return dict(
                status=status,
                x_hat=x_hat,
                theta_hat=theta_hat,
                y_hat=y_hat,
                true_val=true_val,
                viol=gap,
                integral=is_int,
                cuts_added=total_new_cuts,
                bulge_added=bulge_added,
                submod_added=submod_added,
            )

        # é•åã‚ã‚Š â†’ ã‚«ãƒƒãƒˆ 1 æœ¬è¿½åŠ ï¼ˆè¡Œ8ï¼‰
        policy = cut_policy.lower()
        if policy == "submodular" or (policy == "auto" and is_int):
            x_bin = (x_hat > 0.5).astype(float)
            y_bin = (y_hat > 0.5).astype(float)
            c0, coeffs = submodular_cut_coeffs(data, x_bin, y_bin)
            node["cuts"].append((c0, coeffs))
            global_cuts.append((c0, coeffs))
            submod_added += 1
            added_kind = "submod"
        elif policy == "bulge" or (policy == "auto" and not is_int):
            Lb, grad = Lb_and_grad(data, x_hat, y_hat)
            c0 = Lb - float((grad * x_hat).sum())
            coeffs = {j: float(grad[j]) for j in range(J) if abs(grad[j]) > 1e-12}
            node["cuts"].append((c0, coeffs))
            global_cuts.append((c0, coeffs))
            bulge_added += 1
            added_kind = "bulge"
        else:
            raise ValueError(f"unknown cut_policy: {policy}")

        total_new_cuts += 1
        log_cut(added_kind, log_level)

    # ä¸Šé™ã«é”ã—ãŸå ´åˆ
    node["ub"] = float(theta_hat)
    log_warn("ã“ã®ãƒãƒ¼ãƒ‰ã§ã®ã‚«ãƒƒãƒˆä¸Šé™ã«åˆ°é”ã€‚", log_level)
    return dict(
        status=status,
        x_hat=x_hat,
        theta_hat=theta_hat,
        y_hat=y_hat,
        true_val=true_val,
        viol=gap,
        integral=is_int,
        cuts_added=total_new_cuts,
        bulge_added=bulge_added,
        submod_added=submod_added,
    )


# ========== ãƒ¡ã‚¤ãƒ³ï¼šå®Œå…¨ Branch-and-Cutï¼ˆè«–æ–‡ Algorithm 1ï¼‰ ====================


def scflp_branch_and_cut(
    data: Dict[str, object],
    max_nodes: int = 30,
    max_rounds_per_node: int = 200,
    tol: float = 1e-12,
    pulp_solver: str = "CBC",
    node_selection: str = "dfs",  # 'dfs' or 'bestbound'
    log_level: str = "info",
    separation: str = "approx",
    cut_policy: str = "auto",
    lp_relax: bool = True,
) -> Dict[str, object]:
    """
    è«–æ–‡ Algorithm 1 ã«æ²¿ã£ãŸ Branch-and-Cut å®Ÿè£…ã€‚
    """
    J = int(data["J"])
    p = int(data["p"])
    t0 = time.time()

    # ä¸‹ç•Œãƒ»ãƒ™ã‚¹ãƒˆè§£ï¼ˆè¡Œ2ï¼‰
    theta_LB = 0.0
    x_best = None

    total_bulge = 0
    total_submod = 0
    total_cuts = 0

    # ãƒ•ã‚©ãƒ¼ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é›†åˆ Fï¼ˆè¡Œ1: ç·©å’Œã‚’1ã¤å…¥ã‚Œã¦åˆæœŸåŒ–ï¼‰
    nodes: List[Dict[str, object]] = [make_node()]
    global_cuts: List[Tuple[float, Dict[int, float]]] = []

    explored = 0
    log_header("Branch-and-Cutï¼ˆè«–æ–‡ Algorithm 1ï¼‰é–‹å§‹", log_level)

    while nodes and explored < max_nodes:
        # ãƒ•ã‚©ãƒ¼ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å–ã‚Šå‡ºã—ï¼ˆè¡Œ3â€“4ï¼‰
        if node_selection == "bestbound":
            idx = int(np.argmax([n.get("ub", -1e100) for n in nodes]))
            node = nodes.pop(idx)
        else:
            node = nodes.pop()

        explored += 1
        log_line(log_level)
        log_step(
            f"ğŸŒ¿ ãƒãƒ¼ãƒ‰å±•é–‹: depth={node['depth']}, |fix1|={len(node['fix1'])}, |fix0|={len(node['fix0'])}",
            log_level,
        )

        # æ—©æœŸãƒã‚§ãƒƒã‚¯: |fix1| > p ã¯ä¸å¯èƒ½
        if len(node["fix1"]) > p:
            log_warn("æ—©æœŸæåˆˆã‚Š: |fix1| > p", log_level)
            continue

        # è¡Œ5â€“9: é•åãŒç„¡ããªã‚‹ã¾ã§ã“ã®ãƒãƒ¼ãƒ‰ã‚’å¼·åŒ–ã—ã¦è§£ã
        info = node_solve_until_no_violation(
            data=data,
            node=node,
            global_cuts=global_cuts,
            pulp_solver=pulp_solver,
            tol=tol,
            cut_policy=cut_policy,
            separation=separation,
            max_cuts_per_node=max_rounds_per_node,
            log_level=log_level,
            lp_relax=True,
        )
        theta_hat = info["theta_hat"]
        x_hat = info["x_hat"]
        is_int = info["integral"]
        node["ub"] = float(theta_hat)

        total_cuts += int(info.get("cuts_added", 0))
        total_bulge += int(info.get("bulge_added", 0))
        total_submod += int(info.get("submod_added", 0))

        # è¡Œ10â€“13: é•åãŒç„¡ããªã£ãŸå¾Œã®åˆ†å²
        if theta_hat > theta_LB + tol and is_int:
            # è¡Œ10â€“11: ã‚¤ãƒ³ã‚«ãƒ³ãƒ™ãƒ³ãƒˆæ›´æ–°
            theta_LB = float(theta_hat)
            x_best = (x_hat > 0.5).astype(float)
            log_success(f"ğŸ¯ ã‚¤ãƒ³ã‚«ãƒ³ãƒ™ãƒ³ãƒˆæ›´æ–°: Î¸_LB â† {theta_LB:.6f}", log_level)
            continue

        if theta_hat > theta_LB + tol and not is_int:
            # è¡Œ12â€“13: åˆ†æã—ã¦ 2 ã¤ã®ãƒ•ã‚©ãƒ¼ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ F ã¸
            j_star = choose_branch_variable(x_hat)
            if j_star is None:
                # å¿µã®ãŸã‚ã®ä¿é™º
                theta_LB = max(theta_LB, float(theta_hat))
                x_best = (x_hat > 0.5).astype(float)
                log_warn("x ã¯å®Ÿè³ªæ•´æ•°ã ã£ãŸãŸã‚æ›´æ–°ã®ã¿ã€‚", log_level)
                continue

            child0 = make_node(
                fix1=node["fix1"],
                fix0=node["fix0"] | {j_star},
                cuts=node["cuts"],
                depth=node["depth"] + 1,
            )
            child1 = make_node(
                fix1=node["fix1"] | {j_star},
                fix0=node["fix0"],
                cuts=node["cuts"],
                depth=node["depth"] + 1,
            )
            # æ—©æœŸä¸å¯èƒ½ï¼ˆ|fix1|>pï¼‰ã¯æŠ•å…¥å‰ã«é™¤å¤–
            if len(child1["fix1"]) <= p:
                nodes.append(child1)
            else:
                log_warn(f"å­ãƒãƒ¼ãƒ‰(x[{j_star}]=1)ã¯ |fix1|>p ã®ãŸã‚ç ´æ£„", log_level)
            nodes.append(child0)
            log_step(
                f"ğŸŒ± åˆ†æ: j*={j_star} â†’ å­ãƒãƒ¼ãƒ‰ depth={node['depth']+1} ã‚’2ã¤è¿½åŠ ",
                log_level,
            )
            continue

        # Î¸Ì‚ ãŒä¸‹ç•Œä»¥ä¸‹ãªã‚‰æåˆˆã‚Š
        log_step(f"ğŸª“ æåˆˆã‚Š: Î¸Ì‚={theta_hat:.6f} â‰¤ Î¸_LB={theta_LB:.6f}", log_level)

    # çµ‚äº†
    elapsed = time.time() - t0
    gap = 0.0
    if nodes:
        gap = max([n.get("ub", -1e100) for n in nodes]) - theta_LB
    status = "optimal" if not nodes else "stopped_or_pruned"

    log_done("Branch-and-Cut çµ‚äº†", t0, log_level)
    if x_best is not None:
        sel = np.where(x_best > 0.5)[0].tolist()
        log_step(f"âœ¨ æœ€è‰¯è§£ Î¸ = {theta_LB:.6f} / é¸æŠã‚µã‚¤ãƒˆ {sel}", log_level)
    else:
        log_warn("å®Ÿè¡Œä¸­ã«ã‚¤ãƒ³ã‚«ãƒ³ãƒ™ãƒ³ãƒˆã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", log_level)

    log_step(
        f"ğŸ§¾ ã‚«ãƒƒãƒˆç·è¨ˆ: {total_cuts} æœ¬ï¼ˆBulge: {total_bulge}, Submodular: {total_submod}ï¼‰",
        log_level,
    )

    return dict(
        x_best=x_best,
        theta_best=float(theta_LB),
        status=status,
        nodes_explored=explored,
        time_sec=elapsed,
        gap_bound=float(gap),
        total_cuts=total_cuts,
        total_bulge=total_bulge,
        total_submod=total_submod,
    )


# =========ï¼ˆå‚è€ƒï¼‰ä»¥ä¸‹ã¯é«˜åº¦ãªå³å¯†åˆ†é›¢ã®å®Ÿè£…è£œåŠ©ï¼ˆæœªä½¿ç”¨ï¼‰======================
# â€» Algorithm 1 ã«å«ã‚ãªã„ãŸã‚ã€ä½¿ã‚ãšã«æ®‹ã—ã¦ã„ã¾ã™ï¼ˆå¿…è¦ãªã‚‰å‘¼ã³å‡ºã—ã¦ãã ã•ã„ï¼‰ã€‚


def _Ly_S_and_union_allk(
    data: Dict[str, object], S_bin: np.ndarray, y_bin: np.ndarray
) -> Tuple[float, np.ndarray]:
    w = data["w"]
    UL = data["UL"]
    UF = data["UF"]
    h = data["h"]
    S_bin = S_bin.reshape(-1)
    y_bin = y_bin.reshape(-1)
    I, J = w.shape

    wS = w.dot(S_bin)
    wSY = w.dot(np.maximum(S_bin, y_bin))

    num_S = UL + wS
    den_S = UL + UF + wSY
    Ly_S = float((h * (num_S / den_S)).sum())

    add_S = 1.0 - S_bin
    add_SY = 1.0 - np.maximum(S_bin, y_bin)

    num_all = num_S[:, None] + w * add_S[None, :]
    den_all = den_S[:, None] + w * add_SY[None, :]
    Ly_SuK = (h[:, None] * (num_all / den_all)).sum(axis=0)
    return Ly_S, Ly_SuK


def _rho_full_minus_k_vector(
    data: Dict[str, object], y_bin: np.ndarray
) -> Tuple[float, np.ndarray]:
    w = data["w"]
    UL = data["UL"]
    UF = data["UF"]
    h = data["h"]
    y_bin = y_bin.reshape(-1)
    I, J = w.shape

    onesJ = np.ones(J)
    wJ = w.dot(onesJ)
    num_J = UL + wJ
    den_J = UL + UF + wJ
    Ly_J = float((h * (num_J / den_J)).sum())

    y_bar = 1.0 - y_bin
    num_J_minus_k = UL[:, None] + wJ[:, None] - w
    den_J_minus_k = UL[:, None] + UF[:, None] + wJ[:, None] - (y_bar[None, :] * w)
    Ly_J_minus_k = (h[:, None] * (num_J_minus_k / den_J_minus_k)).sum(axis=0)
    rho_full = Ly_J - Ly_J_minus_k
    return Ly_J, rho_full


def _H_value(
    data: Dict[str, object],
    x: np.ndarray,
    y_bin: np.ndarray,
    S_bin: np.ndarray,
    pre: Dict[str, object],
) -> float:
    Ly_S, Ly_SuK = _Ly_S_and_union_allk(data, S_bin, y_bin)
    p = pre["p"]
    rho_full = pre["rho_full"]
    term1 = Ly_S
    term2 = -float(((rho_full * (1.0 - x)) * S_bin).sum())
    term3 = float(x.dot(Ly_SuK)) - p * Ly_S
    return term1 + term2 + term3


def _H_empty(data: Dict[str, object], x: np.ndarray, y_bin: np.ndarray) -> float:
    J = int(data["J"])
    S0 = np.zeros(J)
    Ly_0, Ly_0_u_k = _Ly_S_and_union_allk(data, S0, y_bin)
    p = int(data["p"])
    return float(Ly_0 + x.dot(Ly_0_u_k) - p * Ly_0)


def _greedy_extreme_point_for_baseF(
    data: Dict[str, object],
    x: np.ndarray,
    y_bin: np.ndarray,
    weights: np.ndarray,
    pre: Dict[str, object],
) -> np.ndarray:
    J = int(data["J"])
    order = np.argsort(weights)  # æ˜‡é †
    S_bin = np.zeros(J)
    v = np.zeros(J)
    F_prev = 0.0

    for idx in order:
        S_bin[idx] = 1.0
        H_curr = _H_value(data, x, y_bin, S_bin, pre)
        F_curr = H_curr - pre["H_empty"]
        v[idx] = F_curr - F_prev
        F_prev = F_curr

    s = v.sum()
    FJ = _H_value(data, x, y_bin, np.ones(J), pre) - pre["H_empty"]
    if abs(s - FJ) > 1e-9:
        v[order[-1]] += FJ - s
    return v


def _mnp_min_norm_point(
    data: Dict[str, object],
    x: np.ndarray,
    y_bin: np.ndarray,
    pre: Dict[str, object],
    max_iter: int = 60,
    eps: float = 1e-8,
) -> np.ndarray:
    J = int(data["J"])
    rng = np.random.default_rng(0)
    w0 = rng.normal(size=J)
    v0 = _greedy_extreme_point_for_baseF(data, x, y_bin, w0, pre)

    V = [v0]
    lamb = np.array([1.0])
    y = v0.copy()

    for _ in range(max_iter):
        v = _greedy_extreme_point_for_baseF(data, x, y_bin, y, pre)
        if float(y.dot(y - v)) <= eps:
            return y
        V.append(v)
        m = len(V)
        Vmat = np.column_stack(V)
        alpha = np.zeros(m)
        alpha[:-1] = lamb
        alpha[-1] = 0.0

        while True:
            G = Vmat.T @ Vmat
            ones = np.ones(m)
            KKT = np.block([[G, ones[:, None]], [ones[None, :], np.zeros((1, 1))]])
            rhs = np.zeros(m + 1)
            rhs[-1] = 1.0
            sol = np.linalg.lstsq(KKT, rhs, rcond=None)[0]
            lam_new = sol[:m]
            if np.all(lam_new >= -1e-12):
                lamb = lam_new
                y = Vmat @ lamb
                break
            dirv = lam_new - alpha
            bad = dirv < 0
            t = np.min(alpha[bad] / (alpha[bad] - lam_new[bad]))
            alpha = alpha + t * (lam_new - alpha)
            keep = alpha > 1e-12
            V = [V[i] for i in range(m) if keep[i]]
            alpha = alpha[keep]
            Vmat = np.column_stack(V)
            m = len(V)
            lamb = alpha / alpha.sum()
            y = Vmat @ lamb

    return y


def submodular_cut_coeffs_fractional_exact(
    data: Dict[str, object],
    x: np.ndarray,
    y_hat: np.ndarray,
) -> Tuple[float, Dict[int, float], float, np.ndarray]:
    """åˆ†æ•° x ç”¨ã®å³å¯† NW ã‚«ãƒƒãƒˆï¼ˆå‚è€ƒå®Ÿè£…; Algorithm 1 ã§ã¯ä½¿ç”¨ã—ãªã„ï¼‰ã€‚"""
    J = int(data["J"])
    x = x.reshape(J)
    y_bin = (y_hat > 0.5).astype(float).reshape(J)

    Ly_J, rho_full = _rho_full_minus_k_vector(data, y_bin)
    H0 = _H_empty(data, x, y_bin)
    pre = dict(Ly_J=Ly_J, rho_full=rho_full, H_empty=H0, p=int(data["p"]))

    y_mnp = _mnp_min_norm_point(data, x, y_bin, pre)
    S_star = (y_mnp < 0.0 - 1e-12).astype(float)
    S_idx = np.where(S_star > 0.5)[0]

    Ly_S, Ly_SuK = _Ly_S_and_union_allk(data, S_star, y_bin)
    coeffs: Dict[int, float] = {}
    for k in S_idx:
        coeffs[int(k)] = float(rho_full[k])
    notS = np.where(S_star < 0.5)[0]
    rho_S_k = Ly_SuK[notS] - Ly_S
    for k, val in zip(notS, rho_S_k):
        coeffs[int(k)] = float(val)

    c0 = float(Ly_S - rho_full[S_idx].sum())
    rhs_val = c0 + float(sum(coeffs[j] * x[j] for j in range(J)))
    return c0, coeffs, rhs_val, S_idx
